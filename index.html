<!-- HTML header for doxygen 1.9.1-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.10.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>LLM for Unity: Overview</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link rel="shortcut icon" href="logo_tiny.png" type="image/png">
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<!-- ... other metadata & script includes ... -->
<script type="text/javascript" src="doxygen-awesome-fragment-copy-button.js"></script>
<script type="text/javascript" src="doxygen-awesome-darkmode-toggle.js"></script>
<script type="text/javascript" src="doxygen-awesome-paragraph-link.js"></script>
<script type="text/javascript" src="doxygen-awesome-interactive-toc.js"></script>
<script type="text/javascript">
    DoxygenAwesomeFragmentCopyButton.init()
    DoxygenAwesomeDarkModeToggle.init()
    DoxygenAwesomeParagraphLink.init()
    DoxygenAwesomeInteractiveToc.init()
</script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="custom.css" rel="stylesheet" type="text/css"/>
<link href="doxygen-awesome-sidebar-only.css" rel="stylesheet" type="text/css"/>
<link href="doxygen-awesome-sidebar-only-darkmode-toggle.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<!-- https://tholman.com/github-corners/ -->
<a href="https://github.com/undreamai/LLMUnity" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="logo_tiny.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">LLM for Unity
   &#160;<span id="projectnumber">v1.2.8</span>
   </div>
   <div id="projectbrief">Create characters in Unity with LLMs!</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.10.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('index.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Overview </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_README"></a></p>
<h1 align="center"><img src="images/logo.png" alt="" height="150" class="inline"/>  </h1>
<h3 align="center">Create characters in Unity with LLMs!</h3>
<p><a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License: MIT" style="pointer-events: none;" class="inline"/></a> <a href="https://discord.gg/RwXKQb6zdv"><img src="https://discordapp.com/api/guilds/1194779009284841552/widget.png?style=shield" alt="" class="inline"/></a> <a href="https://www.reddit.com/user/UndreamAI"><img src="https://img.shields.io/badge/Reddit-%23FF4500.svg?style=flat&amp;logo=Reddit&amp;logoColor=white" alt="Reddit" style="pointer-events: none;" class="inline"/></a> <a href="https://www.linkedin.com/company/undreamai"><img src="https://img.shields.io/badge/LinkedIn-blue?style=flat&amp;logo=linkedin&amp;labelColor=blue" alt="LinkedIn" class="inline"/></a> <a href="https://assetstore.unity.com/packages/slug/273604"><img src="https://img.shields.io/badge/Asset%20Store-black.svg?style=flat&amp;logo=unity" alt="Asset Store" style="pointer-events: none;" class="inline"/></a> <a href="https://github.com/undreamai/LLMUnity"><img src="https://img.shields.io/github/stars/undreamai/LLMUnity?style=flat&amp;logo=github&amp;color=f5f5f5" alt="GitHub Repo stars" class="inline"/></a> <a href="https://undream.ai/LLMUnity"><img src="https://img.shields.io/badge/Docs-white.svg?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAwEAYAAAAHkiXEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABl0RVh0U29mdHdhcmUAd3d3Lmlua3NjYXBlLm9yZ5vuPBoAAATqSURBVHic7ZtbiE1RGMc349K4M5EwklwjzUhJCMmTJPJAYjQXJJcH8+Blkry4lPJA8aAoJbekDLmUS6E8SHJL5AW5JPf77eHv93C22Wfttc/ee+0zc/4vv+bMXvusvfZa3/q+b33H80oqqaSSSmqrKnPdgXjUvbvYq5f4+7f486eb/rRajRsn7t4tPngg/vol/vkj/vghXr0q7tghzpyZ//79+on79omXLombNondukXrd9GoSxdx8mSxqUm8eVNkgAvl0aPioEFip07i6dP52z15Ig4fbvVY2VVFhbhokXjrlogJiWvAg/jwoXjqVO73+leUny9eiFVV5mfMlLDRBw+KX76ISQ+0LZ8/F00v4uJFsWPHFh83O+rdWzx3TnQ9wCZ+/Sqyl5iux1RmTu3aiYcPi64H1pasALypoOv4/8SJXraEbXc9kLbECxo2TKyuFj9/zt9u+XIvG8LWv3wpuh5QW86f3/JznT+fv93s2S23C1Z72wbhtH692LdvMvdPSgzkhAkiJhT16ZO/PRPOmcr+Rda4aa5nclTeuZP7PDgRpr1g40bPrQYOFF0PYKHEC+raVVy8OFy7R49EArvURU4mrUAqaTY0iB8/2rXD+XCm5mbR9QAWylevorV7/VpkL0ld06eLpkiyWPj9u93179+LpFZwZ1PXtGnitWui64GMStPmG7SH1NSIJBNHjvTSFZvRvHlise0N9JcBtW1/44Y4dqx45IjnU0JxAGLpklPx+9VZFwPp/9v/eZDGjxcZh7dv4+mXtch+up7Rca+MsJvxiRNi6nvBhg25HWprZMaPGeOlqxEjxGKz+XGRTAAmyJnq6sR370TXA2NLW+8HNjZ62dLOnaLrAQ1r2zmqPH482n0mTfJCKmEvCJHUooNZE/369Elct06kqiKsONRfulTEFDsX8QDlIa5nup9374pE8IiZHPY+ly+LZE/37/cM6mC6IB6Vl4urV6fzfUG6d0/csyf37wsXRFInaM4ckTjGdPg+apTYs6dI3RIWwH//1DV1qkiuxNY2FzrTd+2y6y8z2HQU6efZs+KBAyJZ4v+V0h6ArlwROaQP0uPH4ooV4sqV8Xz/4MF211M2wwoOq1mzRAq5Pnywa5+4KDHE9mI7ly0TO3fOvZ6/eZCoKwB32HS0SMFV1DNtImBKHYstBROoQ4fEQk2RaS+qrxejmj5M7NatIhWARS82xUJfAKahzFcdPnq0GLYgy7Rnbd8e6rGKRyzpuNzPBQty709RcNSZf/KkuHCh2GpMDyKbGNcLYE+YMkVks336NFx7XhTZ3szXiBaqtWvFuAOxM2dEZiyH8UErgc8JLNun7E0aFffSI7RP6owZmz9kSO73HjsmXr8ukppYsybSYyQvBp5QfOjQ3M9tRR496pGgLf1JtLlzRZJzlFzGp4SWDnUxFCrdvy+uWiWa3DJe3N69oj8uSEq8CER88uaNOGBAOv2ILGY69TBBJoM8O0t72zaRoztXBzlLlrT8XARW/IQq82JTMv3mKmv0/9CC4mJMYPwrMSETxAyurRUxQVmXP1fEid7mzeK3b+n2Jzb16CFu2SIWmtNJiriVxANsyq0uoCJfTk4G9y4t24/bSQ0rTkP6gVTG3mz//uKMGSK/ucId5Xe9lZUi5eMMLGUgz56J5Hxu3xZ50Xg3RMIltVn9BRja26PYsBHgAAAAAElFTkSuQmCC" alt="Documentation" style="pointer-events: none;" class="inline"/></a></p>
<p>LLM for Unity enables seamless integration of Large Language Models (LLMs) within the Unity engine.<br  />
 It allows to create intelligent characters that your players can interact with for an immersive experience.<br  />
 LLM for Unity is built on top of the awesome <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> and <a href="https://github.com/Mozilla-Ocho/llamafile">llamafile</a> libraries.</p>
<h1><a class="anchor" id="autotoc_md1"></a>
At a glance</h1>
<ul>
<li>üíª Cross-platform! Windows, Linux and macOS</li>
<li>üè† Runs locally without internet access. No data ever leave the game!</li>
<li>‚ö° Blazing fast inference on CPU and GPU (Nvidia, AMD, Apple Metal)</li>
<li>ü§ó Support of the major LLM models</li>
<li>üîß Easy to setup, call with a single line of code</li>
<li>üí∞ Free to use for both personal and commercial purposes</li>
</ul>
<p>üß™ Tested on Unity: 2021 LTS, 2022 LTS, 2023<br  />
 üö¶ <a href="https://github.com/orgs/undreamai/projects/2/views/10">Upcoming Releases</a></p>
<h1><a class="anchor" id="autotoc_md2"></a>
How to help</h1>
<ul>
<li><a href="https://github.com/undreamai/LLMUnity">‚≠ê Star the repo</a> and spread the word about the project!</li>
<li>Join us at <a href="https://discord.gg/RwXKQb6zdv">Discord</a> and say hi!</li>
<li>Submit feature requests or bugs as issues or even submit a PR and become a collaborator</li>
</ul>
<h1><a class="anchor" id="autotoc_md3"></a>
Games using LLM for Unity</h1>
<ul>
<li><a href="https://store.steampowered.com/app/2778780/Verbal_Verdict/">Verbal Verdict</a></li>
<li><a href="https://store.steampowered.com/app/2786750/I_Chatbot_AISYLUM">I, Chatbot: AISYLUM</a></li>
<li><a href="https://unicorninteractive.itch.io/nameless-souls-of-the-void">Nameless Souls of the Void</a></li>
<li><a href="https://roadedlich.itch.io/murder-in-aisle-4">Murder in Aisle 4</a></li>
</ul>
<h1><a class="anchor" id="autotoc_md4"></a>
Setup</h1>
<p><em>Method 1: Install using the asset store</em></p><ul>
<li>Open the <a href="https://assetstore.unity.com/packages/slug/273604">LLM for Unity</a> asset page and click <code>Add to My Assets</code></li>
<li>Open the Package Manager in Unity: <code>Window &gt; Package Manager</code></li>
<li>Select the <code>Packages: My Assets</code> option from the drop-down</li>
<li>Select the <code>LLM for Unity</code> package, click <code>Download</code> and then <code>Import</code></li>
</ul>
<p><em>Method 2: Install using the GitHub repo:</em></p><ul>
<li>Open the Package Manager in Unity: <code>Window &gt; Package Manager</code></li>
<li>Click the <code>+</code> button and select <code>Add package from git URL</code></li>
<li>Use the repository URL <code><a href="https://github.com/undreamai/LLMUnity.git">https://github.com/undreamai/LLMUnity.git</a></code> and click <code>Add</code></li>
</ul>
<p>On <em>macOS</em> you need the Xcode Command Line Tools:</p><ul>
<li>From inside a terminal run <code>xcode-select --install</code></li>
</ul>
<h1><a class="anchor" id="autotoc_md5"></a>
How to use</h1>
<p><img src="images/character.png" alt="" height="300" class="inline"/></p>
<p>The first step is to create a GameObject for the LLM ‚ôüÔ∏è:</p><ul>
<li>Create an empty GameObject.<br  />
In the GameObject Inspector click <code>Add Component</code> and select the LLM script.</li>
<li>Download one of the default models with the <code>Download Model</code> button (~GBs).<br  />
Or load your own .gguf model with the <code>Load model</code> button (see Use your own model).</li>
<li>Define the role of your AI in the <code>Prompt</code>. You can also define the name of the AI (<code>AI Name</code>) and the player (<code>Player Name</code>).</li>
<li>(Optional) By default you receive the reply from the model as is it is produced in real-time (recommended).<br  />
If you want the full reply in one go, deselect the <code>Stream</code> option.</li>
<li>(Optional) Adjust the server or model settings to your preference (see Options).</li>
</ul>
<p>In your script you can then use it as follows ü¶Ñ: </p><div class="fragment"><div class="line"><span class="keyword">using </span><a class="code hl_namespace" href="namespaceLLMUnity.html">LLMUnity</a>;</div>
<div class="line"> </div>
<div class="line"><span class="keyword">public</span> <span class="keyword">class </span>MyScript {</div>
<div class="line">  <span class="keyword">public</span> <a class="code hl_class" href="classLLMUnity_1_1LLM.html">LLM</a> llm;</div>
<div class="line">  </div>
<div class="line">  <span class="keywordtype">void</span> HandleReply(<span class="keywordtype">string</span> reply){</div>
<div class="line">    <span class="comment">// do something with the reply from the model</span></div>
<div class="line">    Debug.Log(reply);</div>
<div class="line">  }</div>
<div class="line">  </div>
<div class="line">  <span class="keywordtype">void</span> Game(){</div>
<div class="line">    <span class="comment">// your game function</span></div>
<div class="line">    ...</div>
<div class="line">    <span class="keywordtype">string</span> message = <span class="stringliteral">&quot;Hello bot!&quot;</span>;</div>
<div class="line">    _ = llm.<a class="code hl_function" href="classLLMUnity_1_1LLMClient.html#adf78bb425d542394156740244dbf0458">Chat</a>(message, HandleReply);</div>
<div class="line">    ...</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="ttc" id="aclassLLMUnity_1_1LLMClient_html_adf78bb425d542394156740244dbf0458"><div class="ttname"><a href="classLLMUnity_1_1LLMClient.html#adf78bb425d542394156740244dbf0458">LLMUnity.LLMClient.Chat</a></div><div class="ttdeci">async Task&lt; string &gt; Chat(string query, Callback&lt; string &gt; callback=null, EmptyCallback completionCallback=null, bool addToHistory=true)</div><div class="ttdoc">Chat functionality of the LLM. It calls the LLM completion based on the provided query including the ...</div><div class="ttdef"><b>Definition</b> <a href="LLMClient_8cs_source.html#l00446">LLMClient.cs:446</a></div></div>
<div class="ttc" id="aclassLLMUnity_1_1LLM_html"><div class="ttname"><a href="classLLMUnity_1_1LLM.html">LLMUnity.LLM</a></div><div class="ttdoc">Class implementing the LLM server.</div><div class="ttdef"><b>Definition</b> <a href="LLM_8cs_source.html#l00021">LLM.cs:22</a></div></div>
<div class="ttc" id="anamespaceLLMUnity_html"><div class="ttname"><a href="namespaceLLMUnity.html">LLMUnity</a></div><div class="ttdef"><b>Definition</b> <a href="LLM_8cs_source.html#l00014">LLM.cs:15</a></div></div>
</div><!-- fragment --><p> You can also specify a function to call when the model reply has been completed.<br  />
 This is useful if the <code>Stream</code> option is selected for continuous output from the model (default behaviour): </p><div class="fragment"><div class="line"><span class="keywordtype">void</span> ReplyCompleted(){</div>
<div class="line">  <span class="comment">// do something when the reply from the model is complete</span></div>
<div class="line">  Debug.Log(<span class="stringliteral">&quot;The AI replied&quot;</span>);</div>
<div class="line">}</div>
<div class="line"> </div>
<div class="line"><span class="keywordtype">void</span> Game(){</div>
<div class="line">  <span class="comment">// your game function</span></div>
<div class="line">  ...</div>
<div class="line">  <span class="keywordtype">string</span> message = <span class="stringliteral">&quot;Hello bot!&quot;</span>;</div>
<div class="line">  _ = llm.<a class="code hl_function" href="classLLMUnity_1_1LLMClient.html#adf78bb425d542394156740244dbf0458">Chat</a>(message, HandleReply, ReplyCompleted);</div>
<div class="line">  ...</div>
<div class="line">}</div>
</div><!-- fragment --><p>To stop the chat without waiting its completion you can use: </p><div class="fragment"><div class="line">llm.<a class="code hl_function" href="classLLMUnity_1_1LLMClient.html#a50b5235db5f784c7a0b4f8f3d3e7f2fe">CancelRequests</a>();</div>
<div class="ttc" id="aclassLLMUnity_1_1LLMClient_html_a50b5235db5f784c7a0b4f8f3d3e7f2fe"><div class="ttname"><a href="classLLMUnity_1_1LLMClient.html#a50b5235db5f784c7a0b4f8f3d3e7f2fe">LLMUnity.LLMClient.CancelRequests</a></div><div class="ttdeci">void CancelRequests()</div><div class="ttdoc">Cancel the ongoing requests e.g. Chat, Complete.</div><div class="ttdef"><b>Definition</b> <a href="LLMClient_8cs_source.html#l00591">LLMClient.cs:591</a></div></div>
</div><!-- fragment --><ul>
<li>Finally, in the Inspector of the GameObject of your script, select the LLM GameObject created above as the llm property.</li>
</ul>
<p>That's all ‚ú®! <br  />
<br  />
 You can also:</p>
<details >
<summary >
Build multiple characters</summary>
<p></p>
<p>LLM for Unity allows to build multiple AI characters efficiently, where each character has it own prompt.<br  />
 See the <a href="https://github.com/undreamai/LLMUnity/tree/main/Samples~/ServerClient">ServerClient</a> sample for a server-client example.</p>
<p>To use multiple characters:</p><ul>
<li>create a single GameObject for the LLM as above for the first character.</li>
<li>for every additional character create a GameObject using the LLMClient script instead of the LLM script.<br  />
 Define the prompt (and other parameters) of the LLMClient for the character</li>
</ul>
<p>Then in your script: </p><div class="fragment"><div class="line"><span class="keyword">using </span><a class="code hl_namespace" href="namespaceLLMUnity.html">LLMUnity</a>;</div>
<div class="line"> </div>
<div class="line"><span class="keyword">public</span> <span class="keyword">class </span>MyScript {</div>
<div class="line">  <span class="keyword">public</span> <a class="code hl_class" href="classLLMUnity_1_1LLM.html">LLM</a> cat;</div>
<div class="line">  <span class="keyword">public</span> <a class="code hl_class" href="classLLMUnity_1_1LLMClient.html">LLMClient</a> dog;</div>
<div class="line">  <span class="keyword">public</span> <a class="code hl_class" href="classLLMUnity_1_1LLMClient.html">LLMClient</a> bird;</div>
<div class="line">  </div>
<div class="line">  <span class="keywordtype">void</span> HandleCatReply(<span class="keywordtype">string</span> reply){</div>
<div class="line">    <span class="comment">// do something with the reply from the cat character</span></div>
<div class="line">    Debug.Log(reply);</div>
<div class="line">  }</div>
<div class="line"> </div>
<div class="line">  <span class="keywordtype">void</span> HandleDogReply(<span class="keywordtype">string</span> reply){</div>
<div class="line">    <span class="comment">// do something with the reply from the dog character</span></div>
<div class="line">    Debug.Log(reply);</div>
<div class="line">  }</div>
<div class="line"> </div>
<div class="line">  <span class="keywordtype">void</span> HandleBirdReply(<span class="keywordtype">string</span> reply){</div>
<div class="line">    <span class="comment">// do something with the reply from the bird character</span></div>
<div class="line">    Debug.Log(reply);</div>
<div class="line">  }</div>
<div class="line">  </div>
<div class="line">  <span class="keywordtype">void</span> Game(){</div>
<div class="line">    <span class="comment">// your game function</span></div>
<div class="line">    ...</div>
<div class="line">    _ = cat.<a class="code hl_function" href="classLLMUnity_1_1LLMClient.html#adf78bb425d542394156740244dbf0458">Chat</a>(<span class="stringliteral">&quot;Hi cat!&quot;</span>, HandleCatReply);</div>
<div class="line">    _ = dog.<a class="code hl_function" href="classLLMUnity_1_1LLMClient.html#adf78bb425d542394156740244dbf0458">Chat</a>(<span class="stringliteral">&quot;Hello dog!&quot;</span>, HandleDogReply);</div>
<div class="line">    _ = bird.<a class="code hl_function" href="classLLMUnity_1_1LLMClient.html#adf78bb425d542394156740244dbf0458">Chat</a>(<span class="stringliteral">&quot;Hiya bird!&quot;</span>, HandleBirdReply);</div>
<div class="line">    ...</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="ttc" id="aclassLLMUnity_1_1LLMClient_html"><div class="ttname"><a href="classLLMUnity_1_1LLMClient.html">LLMUnity.LLMClient</a></div><div class="ttdoc">Class implementing the LLM client.</div><div class="ttdef"><b>Definition</b> <a href="LLMClient_8cs_source.html#l00052">LLMClient.cs:53</a></div></div>
</div><!-- fragment --><p></p>
</details>
<details >
<summary >
Process the prompt at the beginning of your app for faster initial processing time</summary>
<p></p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> WarmupCompleted(){</div>
<div class="line">  <span class="comment">// do something when the warmup is complete</span></div>
<div class="line">  Debug.Log(<span class="stringliteral">&quot;The AI is warm&quot;</span>);</div>
<div class="line">}</div>
<div class="line"> </div>
<div class="line"><span class="keywordtype">void</span> Game(){</div>
<div class="line">  <span class="comment">// your game function</span></div>
<div class="line">  ...</div>
<div class="line">  _ = llm.<a class="code hl_function" href="classLLMUnity_1_1LLMClient.html#a25549a9833f3a3e4254cd17c7d3ccd4d">Warmup</a>(WarmupCompleted);</div>
<div class="line">  ...</div>
<div class="line">}</div>
<div class="ttc" id="aclassLLMUnity_1_1LLMClient_html_a25549a9833f3a3e4254cd17c7d3ccd4d"><div class="ttname"><a href="classLLMUnity_1_1LLMClient.html#a25549a9833f3a3e4254cd17c7d3ccd4d">LLMUnity.LLMClient.Warmup</a></div><div class="ttdeci">async Task&lt; string &gt; Warmup(EmptyCallback completionCallback=null, string query=&quot;hi&quot;)</div><div class="ttdoc">Allow to warm-up a model by processing the prompt. The prompt processing will be cached (if cacheProm...</div><div class="ttdef"><b>Definition</b> <a href="LLMClient_8cs_source.html#l00530">LLMClient.cs:530</a></div></div>
</div><!-- fragment --><p></p>
</details>
<details >
<summary >
Add or not the message to the chat/prompt history</summary>
<p></p>
<p>The last argument of the <code>Chat</code> function is a boolean that specifies whether to add the message to the history (default: true): </p><div class="fragment"><div class="line"><span class="keywordtype">void</span> Game(){</div>
<div class="line">  <span class="comment">// your game function</span></div>
<div class="line">  ...</div>
<div class="line">  <span class="keywordtype">string</span> message = <span class="stringliteral">&quot;Hello bot!&quot;</span>;</div>
<div class="line">  _ = llm.<a class="code hl_function" href="classLLMUnity_1_1LLMClient.html#adf78bb425d542394156740244dbf0458">Chat</a>(message, HandleReply, ReplyCompleted, <span class="keyword">false</span>);</div>
<div class="line">  ...</div>
<div class="line">}</div>
</div><!-- fragment --><p></p>
</details>
<details >
<summary >
Use pure text completion</summary>
<p></p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> Game(){</div>
<div class="line">  <span class="comment">// your game function</span></div>
<div class="line">  ...</div>
<div class="line">  <span class="keywordtype">string</span> message = <span class="stringliteral">&quot;The cat is away&quot;</span>;</div>
<div class="line">  _ = llm.<a class="code hl_function" href="classLLMUnity_1_1LLMClient.html#afb93776d4bd9d8bd843896a7c4bf7e65">Complete</a>(message, HandleReply, ReplyCompleted);</div>
<div class="line">  ...</div>
<div class="line">}</div>
<div class="ttc" id="aclassLLMUnity_1_1LLMClient_html_afb93776d4bd9d8bd843896a7c4bf7e65"><div class="ttname"><a href="classLLMUnity_1_1LLMClient.html#afb93776d4bd9d8bd843896a7c4bf7e65">LLMUnity.LLMClient.Complete</a></div><div class="ttdeci">async Task&lt; string &gt; Complete(string prompt, Callback&lt; string &gt; callback=null, EmptyCallback completionCallback=null)</div><div class="ttdoc">Pure completion functionality of the LLM. It calls the LLM completion based solely on the provided pr...</div><div class="ttdef"><b>Definition</b> <a href="LLMClient_8cs_source.html#l00508">LLMClient.cs:508</a></div></div>
</div><!-- fragment --><p></p>
</details>
<details >
<summary >
Wait for the reply before proceeding to the next lines of code</summary>
<p></p>
<p>For this you can use the <code>async</code>/<code>await</code> functionality: </p><div class="fragment"><div class="line">async <span class="keywordtype">void</span> Game(){</div>
<div class="line">  <span class="comment">// your game function</span></div>
<div class="line">  ...</div>
<div class="line">  <span class="keywordtype">string</span> message = <span class="stringliteral">&quot;Hello bot!&quot;</span>;</div>
<div class="line">  <span class="keywordtype">string</span> reply = await llm.<a class="code hl_function" href="classLLMUnity_1_1LLMClient.html#adf78bb425d542394156740244dbf0458">Chat</a>(message, HandleReply, ReplyCompleted);</div>
<div class="line">  Debug.Log(reply);</div>
<div class="line">  ...</div>
<div class="line">}</div>
</div><!-- fragment --><p></p>
</details>
<details >
<summary >
Add a LLM / LLMClient component dynamically</summary>
<p></p>
<div class="fragment"><div class="line"><span class="keyword">using </span>UnityEngine;</div>
<div class="line"><span class="keyword">using </span><a class="code hl_namespace" href="namespaceLLMUnity.html">LLMUnity</a>;</div>
<div class="line"> </div>
<div class="line"><span class="keyword">public</span> <span class="keyword">class </span>MyScript : MonoBehaviour</div>
<div class="line">{</div>
<div class="line">    <a class="code hl_class" href="classLLMUnity_1_1LLM.html">LLM</a> llm;</div>
<div class="line">    <a class="code hl_class" href="classLLMUnity_1_1LLMClient.html">LLMClient</a> llmclient;</div>
<div class="line"> </div>
<div class="line">    async <span class="keywordtype">void</span> Start()</div>
<div class="line">    {</div>
<div class="line">        <span class="comment">// Add and setup a LLM object</span></div>
<div class="line">        gameObject.SetActive(<span class="keyword">false</span>);</div>
<div class="line">        llm = gameObject.AddComponent&lt;<a class="code hl_class" href="classLLMUnity_1_1LLM.html">LLM</a>&gt;();</div>
<div class="line">        await llm.<a class="code hl_function" href="classLLMUnity_1_1LLM.html#a244797da23b679c6dc74dc886fe7648b">SetModel</a>(<span class="stringliteral">&quot;mistral-7b-instruct-v0.2.Q4_K_M.gguf&quot;</span>);</div>
<div class="line">        llm.prompt = <span class="stringliteral">&quot;A chat between a curious human and an artificial intelligence assistant.&quot;</span>;</div>
<div class="line">        gameObject.SetActive(<span class="keyword">true</span>);</div>
<div class="line"> </div>
<div class="line">        <span class="comment">// or a LLMClient object</span></div>
<div class="line">        gameObject.SetActive(<span class="keyword">false</span>);</div>
<div class="line">        llmclient = gameObject.AddComponent&lt;<a class="code hl_class" href="classLLMUnity_1_1LLMClient.html">LLMClient</a>&gt;();</div>
<div class="line">        llmclient.prompt = <span class="stringliteral">&quot;A chat between a curious human and an artificial intelligence assistant.&quot;</span>;</div>
<div class="line">        gameObject.SetActive(<span class="keyword">true</span>);</div>
<div class="line">    }</div>
<div class="line">}</div>
<div class="ttc" id="aclassLLMUnity_1_1LLM_html_a244797da23b679c6dc74dc886fe7648b"><div class="ttname"><a href="classLLMUnity_1_1LLM.html#a244797da23b679c6dc74dc886fe7648b">LLMUnity.LLM.SetModel</a></div><div class="ttdeci">async Task SetModel(string path)</div><div class="ttdoc">Allows to set the model used by the LLM. The model provided is copied to the Assets/StreamingAssets f...</div><div class="ttdef"><b>Definition</b> <a href="LLM_8cs_source.html#l00157">LLM.cs:157</a></div></div>
</div><!-- fragment --><p></p>
</details>
<details >
<summary >
Use a remote server</summary>
<p></p>
<p>You can also build a remote server that does the processing and have local clients that interact with it.To do that:</p><ul>
<li>Create a server based on the <code>LLM</code> script or a standard <a href="https://github.com/ggerganov/llama.cpp/blob/master/examples/server">llama.cpp server</a>.</li>
<li>If using the <code>LLM</code> script for the server, enable the <code>Remote</code> option (Advanced options)</li>
<li>Create characters with the <code>LLMClient</code> script. The characters can be configured to connect to the remote instance by providing the IP address (starting with "http://") and port of the server in the <code>host</code>/<code>port</code> properties.</li>
</ul>
<p></p>
</details>
<p>A <b>detailed documentation</b> on function level can be found here: <a href="https://undream.ai/LLMUnity"><img src="https://img.shields.io/badge/Documentation-white.svg?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAwEAYAAAAHkiXEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABl0RVh0U29mdHdhcmUAd3d3Lmlua3NjYXBlLm9yZ5vuPBoAAATqSURBVHic7ZtbiE1RGMc349K4M5EwklwjzUhJCMmTJPJAYjQXJJcH8+Blkry4lPJA8aAoJbekDLmUS6E8SHJL5AW5JPf77eHv93C22Wfttc/ee+0zc/4vv+bMXvusvfZa3/q+b33H80oqqaSSSmqrKnPdgXjUvbvYq5f4+7f486eb/rRajRsn7t4tPngg/vol/vkj/vghXr0q7tghzpyZ//79+on79omXLombNondukXrd9GoSxdx8mSxqUm8eVNkgAvl0aPioEFip07i6dP52z15Ig4fbvVY2VVFhbhokXjrlogJiWvAg/jwoXjqVO73+leUny9eiFVV5mfMlLDRBw+KX76ISQ+0LZ8/F00v4uJFsWPHFh83O+rdWzx3TnQ9wCZ+/Sqyl5iux1RmTu3aiYcPi64H1pasALypoOv4/8SJXraEbXc9kLbECxo2TKyuFj9/zt9u+XIvG8LWv3wpuh5QW86f3/JznT+fv93s2S23C1Z72wbhtH692LdvMvdPSgzkhAkiJhT16ZO/PRPOmcr+Rda4aa5nclTeuZP7PDgRpr1g40bPrQYOFF0PYKHEC+raVVy8OFy7R49EArvURU4mrUAqaTY0iB8/2rXD+XCm5mbR9QAWylevorV7/VpkL0ld06eLpkiyWPj9u93179+LpFZwZ1PXtGnitWui64GMStPmG7SH1NSIJBNHjvTSFZvRvHlise0N9JcBtW1/44Y4dqx45IjnU0JxAGLpklPx+9VZFwPp/9v/eZDGjxcZh7dv4+mXtch+up7Rca+MsJvxiRNi6nvBhg25HWprZMaPGeOlqxEjxGKz+XGRTAAmyJnq6sR370TXA2NLW+8HNjZ62dLOnaLrAQ1r2zmqPH482n0mTfJCKmEvCJHUooNZE/369Elct06kqiKsONRfulTEFDsX8QDlIa5nup9374pE8IiZHPY+ly+LZE/37/cM6mC6IB6Vl4urV6fzfUG6d0/csyf37wsXRFInaM4ckTjGdPg+apTYs6dI3RIWwH//1DV1qkiuxNY2FzrTd+2y6y8z2HQU6efZs+KBAyJZ4v+V0h6ArlwROaQP0uPH4ooV4sqV8Xz/4MF211M2wwoOq1mzRAq5Pnywa5+4KDHE9mI7ly0TO3fOvZ6/eZCoKwB32HS0SMFV1DNtImBKHYstBROoQ4fEQk2RaS+qrxejmj5M7NatIhWARS82xUJfAKahzFcdPnq0GLYgy7Rnbd8e6rGKRyzpuNzPBQty709RcNSZf/KkuHCh2GpMDyKbGNcLYE+YMkVks336NFx7XhTZ3szXiBaqtWvFuAOxM2dEZiyH8UErgc8JLNun7E0aFffSI7RP6owZmz9kSO73HjsmXr8ukppYsybSYyQvBp5QfOjQ3M9tRR496pGgLf1JtLlzRZJzlFzGp4SWDnUxFCrdvy+uWiWa3DJe3N69oj8uSEq8CER88uaNOGBAOv2ILGY69TBBJoM8O0t72zaRoztXBzlLlrT8XARW/IQq82JTMv3mKmv0/9CC4mJMYPwrMSETxAyurRUxQVmXP1fEid7mzeK3b+n2Jzb16CFu2SIWmtNJiriVxANsyq0uoCJfTk4G9y4t24/bSQ0rTkP6gVTG3mz//uKMGSK/ucId5Xe9lZUi5eMMLGUgz56J5Hxu3xZ50Xg3RMIltVn9BRja26PYsBHgAAAAAElFTkSuQmCC" alt="" style="pointer-events: none;" class="inline"/></a></p>
<h1><a class="anchor" id="autotoc_md6"></a>
Examples</h1>
<p>The <a href="https://github.com/undreamai/LLMUnity/tree/main/Samples~">Samples~</a> folder contains several examples of interaction ü§ñ:</p><ul>
<li><a href="https://github.com/undreamai/LLMUnity/tree/main/Samples~/SimpleInteraction">SimpleInteraction</a>: Demonstrates simple interaction between a player and a AI</li>
<li><a href="https://github.com/undreamai/LLMUnity/tree/main/Samples~/AsyncStartup">AsyncStartup</a>: Demonstrates how to use the async functionality for a loading screen</li>
<li><a href="https://github.com/undreamai/LLMUnity/tree/main/Samples~/ServerClient">ServerClient</a>: Demonstrates simple interaction between a player and multiple AIs using a <code>LLM</code> and a <code>LLMClient</code></li>
<li><a href="https://github.com/undreamai/LLMUnity/tree/main/Samples~/ChatBot">ChatBot</a>: Demonstrates interaction between a player and a AI with a UI similar to a messaging app (see image below)</li>
</ul>
<p><img src="images/demo.gif" alt="" width="400" class="inline"/></p>
<p>To install a sample:</p><ul>
<li>Open the Package Manager: <code>Window &gt; Package Manager</code></li>
<li>Select the <code>LLM for Unity</code> Package. From the <code>Samples</code> Tab, click <code>Import</code> next to the sample you want to install.</li>
</ul>
<p>The samples can be run with the <code>Scene.unity</code> scene they contain inside their folder.<br  />
 In the scene, select the <code>LLM</code> GameObject and click the <code>Download Model</code> button to download the default model.<br  />
 You can also load your own model in .gguf format with the <code>Load model</code> button (see Use your own model).<br  />
 Save the scene, run and enjoy!</p>
<h1><a class="anchor" id="autotoc_md7"></a>
Use your own model</h1>
<p>LLM for Unity uses the <a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2">Mistral 7B Instruct</a>, <a href="https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B">OpenHermes 2.5</a> or <a href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf">Microsoft Phi-3</a> model by default, quantised with the Q4 method.<br  />
</p>
<p>Alternative models can be downloaded from <a href="https://huggingface.co/models?library=gguf&amp;sort=downloads">HuggingFace</a>.<br  />
 The required model format is .gguf as defined by the llama.cpp.<br  />
 Other model formats can be converted to gguf with the <code>convert.py</code> script of the llama.cpp as described <a href="https://github.com/ggerganov/llama.cpp/tree/master?tab=readme-ov-file#prepare-data--run">here</a>.<br  />
</p>
<p>‚ùï Before using any model make sure you <b>check their license</b> ‚ùï</p>
<h1><a class="anchor" id="autotoc_md8"></a>
Options</h1>
<ul>
<li><code>Show/Hide Advanced Options</code> Toggle to show/hide advanced options from below</li>
<li><code>Show/Hide Expert Options</code> Toggle to show/hide expert options from below</li>
</ul>
<h3><a class="anchor" id="autotoc_md9"></a>
üíª Server Settings</h3>
<div> <img src="images/GameObject.png" alt="" width="300" align="right" class="inline"/> </div><ul>
<li><code>Num Threads</code> number of threads to use (default: -1 = all)</li>
<li><code>Num GPU Layers</code> number of model layers to offload to the GPU. If set to 0 the GPU is not used. Use a large number i.e. &gt;30 to utilise the GPU as much as possible. If the user's GPU is not supported, the LLM will fall back to the CPU</li>
<li><code>Stream</code> select to receive the reply from the model as it is produced (recommended!).<br  />
 If it is not selected, the full reply from the model is received in one go</li>
<li><details >
<summary >
Advanced options</summary>
<ul>
<li><code>Parallel Prompts</code> number of prompts that can happen in parallel (default: -1 = number of LLM/LLMClient objects)</li>
<li><code>Debug</code> select to log the output of the model in the Unity Editor</li>
<li><code>Asynchronous Startup</code> allows to start the server asynchronously</li>
<li><code>Remote</code> select to allow remote access to the server</li>
<li><code>Port</code> port to run the server</li>
<li><code>Kill Existing Servers On Start</code> kills existing servers by the Unity project on startup to handle Unity crashes</li>
</ul>
<p></p>
</details>
</li>
</ul>
<h3><a class="anchor" id="autotoc_md10"></a>
ü§ó Model Settings</h3>
<ul>
<li><code>Download model</code> click to download one of the default models</li>
<li><code>Load model</code> click to load your own model in .gguf format</li>
<li><code>Model</code> the path of the model being used (relative to the Assets/StreamingAssets folder)</li>
<li><details >
<summary >
<code>Chat Template</code> the chat template to use for constructing the prompts</summary>
<p>The chat template is determined automatically by the chat template of the model (if it exists) or the model name. The "chatml" and "alpaca" templates work with most of the models.</p>
</details>
</li>
<li><details >
<summary >
Advanced options</summary>
<ul>
<li><code>Load lora</code> click to load a LORA model in .bin format</li>
<li><code>Load grammar</code> click to load a grammar in .gbnf format</li>
<li><code>Lora</code> the path of the Lora being used (relative to the Assets/StreamingAssets folder)</li>
<li><code>Grammar</code> the path of the Grammar being used (relative to the Assets/StreamingAssets folder)</li>
<li><details >
<summary >
<code>Context Size</code> size of the prompt context (0 = context size of the model)</summary>
<p>This is the number of tokens the model can take as input when generating responses.</p>
</details>
</li>
<li><code>Batch Size</code> batch size for prompt processing (default: 512)</li>
<li><code>Seed</code> seed for reproducibility. For random results every time select -1</li>
<li><details >
<summary >
<code>Cache Prompt</code> save the ongoing prompt from the chat (default: true)</summary>
<p>Saves the prompt as it is being created by the chat to avoid reprocessing the entire prompt every time</p>
</details>
</li>
<li><details >
<summary >
<code>Num Predict</code> number of tokens to predict (default: 256, -1 = infinity, -2 = until context filled)</summary>
<p>This is the amount of tokens the model will maximum predict. When N predict is reached the model will stop generating. This means words / sentences might not get finished if this is too low. </p>
</details>
</li>
<li><details >
<summary >
<code>Temperature</code> LLM temperature, lower values give more deterministic answers (default: 0.2)</summary>
<p>The temperature setting adjusts how random the generated responses are. Turning it up makes the generated choices more varied and unpredictable. Turning it down makes the generated responses more predictable and focused on the most likely options.</p>
</details>
</li>
<li><details >
<summary >
<code>Top K</code> top-k sampling (default: 40, 0 = disabled)</summary>
<p>The top k value controls the top k most probable tokens at each step of generation. This value can help fine tune the output and make this adhere to specific patterns or constraints.</p>
</details>
</li>
<li><details >
<summary >
<code>Top P</code> top-p sampling (default: 0.9, 1.0 = disabled)</summary>
<p>The top p value controls the cumulative probability of generated tokens. The model will generate tokens until this theshold (p) is reached. By lowering this value you can shorten output &amp; encourage / discourage more diverse output.</p>
</details>
</li>
<li><details >
<summary >
<code>Min P</code> minimum probability for a token to be used (default: 0.05)</summary>
<p>The probability is defined relative to the probability of the most likely token.</p>
</details>
</li>
<li><details >
<summary >
<code>Repeat Penalty</code> control the repetition of token sequences in the generated text (default: 1.1)</summary>
<p>The penalty is applied to repeated tokens.</p>
</details>
</li>
<li><details >
<summary >
<code>Presence Penalty</code> repeated token presence penalty (default: 0.0, 0.0 = disabled)</summary>
<p>Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p>
</details>
</li>
<li><details >
<summary >
<code>Frequency Penalty</code> repeated token frequency penalty (default: 0.0, 0.0 = disabled)</summary>
<p>Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p>
</details>
</li>
</ul>
</details>
</li>
<li><details >
<summary >
Expert options</summary>
<ul>
<li><code>Tfs_z</code>: enable tail free sampling with parameter z (default: 1.0, 1.0 = disabled).</li>
<li><code>Typical P</code>: enable locally typical sampling with parameter p (default: 1.0, 1.0 = disabled).</li>
<li><code>Repeat Last N</code>: last n tokens to consider for penalizing repetition (default: 64, 0 = disabled, -1 = ctx-size).</li>
<li><code>Penalize Nl</code>: penalize newline tokens when applying the repeat penalty (default: true).</li>
<li><code>Penalty Prompt</code>: prompt for the purpose of the penalty evaluation. Can be either <code>null</code>, a string or an array of numbers representing tokens (default: <code>null</code> = use original <code>prompt</code>).</li>
<li><code>Mirostat</code>: enable Mirostat sampling, controlling perplexity during text generation (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0).</li>
<li><code>Mirostat Tau</code>: set the Mirostat target entropy, parameter tau (default: 5.0).</li>
<li><code>Mirostat Eta</code>: set the Mirostat learning rate, parameter eta (default: 0.1).</li>
<li><code>N Probs</code>: if greater than 0, the response also contains the probabilities of top N tokens for each generated token (default: 0)</li>
<li><code>Ignore Eos</code>: ignore end of stream token and continue generating (default: false).</li>
</ul>
</details>
</li>
</ul>
<h3><a class="anchor" id="autotoc_md11"></a>
üó®Ô∏è Chat Settings</h3>
<ul>
<li><code>Player Name</code> the name of the player</li>
<li><code>AI Name</code> the name of the AI</li>
<li><code>Prompt</code> description of the AI role</li>
</ul>
<h1><a class="anchor" id="autotoc_md12"></a>
License</h1>
<p>The license of LLM for Unity is MIT (<a class="el" href="md_LICENSE.html">LICENSE.md</a>) and uses third-party software with MIT and Apache licenses (<a class="el" href="md_Third_01Party_01Notices.html">Third Party Notices.md</a>). </p>
</div></div><!-- PageDoc -->
<a href="doxygen_crawl.html"/>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.10.0 </li>
  </ul>
</div>
</body>
</html>
